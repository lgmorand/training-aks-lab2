---
sectionid: logs
sectionclass: h2
title: Get some logs
parent-id: deploy
---

Everything went fine but that's not always the case.

### Retrieve logs

The application is logging some information.

{% collapsible %}

A well-developed application could potentially push logs directly to a repository (e.g., Application Insights) but there are numerous cases where the logs are simple console logs created by the app. You can retrieve them by running the `kubectl logs` command:

```sh
kubectl logs helloworld-75d9b9d44c-qjkg2
```

You should be able to see the logs of the application:

```sh
azure [ ~ ]$ kubectl logs helloworld-5d9bdb967d-8jmhh
Web app listening on port 80
error! not enough icecream
My little secret: I love icecream
```

{% endcollapsible %}

### Look inside

Not necessarily a good practice but sometimes you need to dig a little bit. The application is creating a file in the cluster. Try to retrieve the content of the file without looking at the source code. The documentation may [help](https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/).

> Hint: the file is named **log.txt**

{% collapsible %}

The solution is to connect directly to the container, more exactly to connect inside the container and look for the file. It can be done with the `kubectl exec` command.

```sh
kubectl exec --stdin --tty helloworld-5d9bdb967d-8jmhh -- /bin/bash
```

Once connected, you can list the files and then display the content of the good one:

```sh
azure [ ~ ]$ kubectl exec --stdin --tty helloworld-5d9bdb967d-8jmhh -- /bin/bash

root@helloworld-5d9bdb967d-8jmhh:/app# ls
README.md  deployment.yaml  dockerfile  log.txt  node_modules nodejs package-lock.json  package.json  server.js

root@helloworld-5d9bdb967d-8jmhh:/app# cat log.txt
42 is the answer

root@helloworld-5d9bdb967d-8jmhh:/app# 
```

{% endcollapsible %}

### Debug a non starting deployment

These two previous solutions are fine when a pod starts correctly but what can you do when your pod doesn't even start ?

Edit your deployment.yaml manifest and update the version of the container image. Put a number which does not exist and rerun your pipeline.

Connect to your cluster and watch the status of your deployment. Is something sketchy?
Try to get more information about the (generic) error and then, get more details about the starting steps of the pods.

{% collapsible %}

```sh
kubectl get deploy helloworld
```

Which should show that one pod should be running but 0 are actually working

```sh
azure [ ~ ]$ kubectl get deploy helloworld
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
helloworld   0/1     1            0           2m31s
```

You could run the `kubectl describe` command on the deployment but you would not get enough information. Then since the pod is not running, you should start from the pod.

```sh
kubectl describe pod helloworld-5d9bdb967d-8jmhh
```

Among the different information from the describe command, you should look at events part:

```sh
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  3m50s                  default-scheduler  Successfully assigned default/helloworld-646b549b59-l4ntr to aks-agentpool-14914408-vmss000001
  Warning  Failed     2m27s (x6 over 3m50s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling    2m16s (x4 over 3m50s)  kubelet            Pulling image "acrtestengie.azurecr.io/helloworld:fakeversion"
  Warning  Failed     2m16s (x4 over 3m50s)  kubelet            Failed to pull image "acrtestengie.azurecr.io/helloworld:fakeversion": rpc error: code = NotFound desc = failed to pull and unpack image "acrtestengie.azurecr.io/helloworld:fakeversion": failed to resolve reference "acrtestengie.azurecr.io/helloworld:fakeversion": acrtestengie.azurecr.io/helloworld:fakeversion: not found
  Warning  Failed     2m16s (x4 over 3m50s)  kubelet            Error: ErrImagePull
  Normal   BackOff    2m2s (x7 over 3m50s)   kubelet            Back-off pulling image "acrtestengie.azurecr.io/helloworld:fakeversion"
```

As you can see, there is clearly an event when the image cannot be downloaded because it can't resolve it. It could be a version mismatch, an authentication issue or even a network issue but here, the message is clear that the version is the faulty part.

{% endcollapsible %}

It's not exhaustive but you learned the basics to debug when something goes wrong with your containers.
